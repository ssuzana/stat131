---
title: "STAT 131 - Intro to Probability Theory"
author: "Suzana Șerboi, Ph.D."
institute: Mathematics and Statistics, UCSC
output:
  xaringan::moon_reader:
    css:
    - rutgers.css
    - "tamu-fonts"
    - custom.css
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
subtitle: 'Lecture 7: Covariance, correlation'

---  

### Covariance and correlation

.definition-box[The **covariance** between r.v.s $X$ and $Y$ is:
$$Cov(X,Y) = E((X-E(X))(Y-E(Y))) = E(XY) - E(X)E(Y).$$
]

Intuition: Covariance is a measure of the joint variability of two random variables. If the differences $(X-E(X))$ and $(Y-E(Y))$ have different signs, then the covariance is negative, and if they have the same signs, then the covariance is positive.

.theorem-box[If $X$ and $Y$ are independent, then they are uncorrelated. 
]

NOTE that the converse is FALSE. Example. $X \sim N(0,1)$ and $Y = X^2$. Then: $E(XY) = E(X^3)= 0$ (odd moments of a symmetric distributions are 0). Thus: $Cov(X,Y) = E(XY)- E(X)E(Y) = 0$ but we know they are not independent: one is a function of the other.

---

The covariance has the following properties:

(1) $Cov(X,X)=Var(X)$

(2) $Cov(X,Y)=Cov(Y,X)$

(3) $Cov(X,c)=0$ for any constant $c$.

(4) $Cov(aX,Y)=aCov(X,Y)$ for any constant $a$.

(5) $Cov(X+Y,Z)=Cov(X,Z)+Cov(Y,Z)$

(6) $Cov(X+c,Y)=Cov(X,Y)$

(7) More generally, $Cov(\sum_{i=1}^m a_i X_i,\sum_{j=1}^n b_j Y_j)= \sum_{i=1}^m \sum_{j=1}^n a_i b_j Cov(X_i,Y_j)$.

(8) $Cov(X+Y,Z+W) = Cov(X,Z)+Cov(X,W)+Cov(Y,Z)+Cov(Y,W)$

(9) $Var(X + Y) = Var(X) + Var(Y) + 2Cov(X,Y)$

(10) $Var(X - Y) = Var(X) + Var(Y) - 2Cov(X,Y)$
---

### Correlation

.definition-box[The **correlation** between r.v.s $X$ and $Y$ is:
$$Corr(X,Y)= \frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y))}}.$$
]

Note that shifting and scaling r.v. has no effect on their correlation:

> $$Corr(cX,Y)= \frac{Cov(cX,Y)}{\sqrt{c^2Var(X)Var(Y)}} = Corr(X,Y).$$

Correlation is bounded between -1 and 1, and will be usually written as $\rho_{X,Y}= Corr(X,Y)$.

![](https://cdn.scribbr.com/wp-content/uploads/2021/08/01-correlation-types-1024x415.png)

---

### Multivariate Normal distribution

.definition-box[A k-dimensional random vector $\mathbf{X}= (X_1,\cdots,X_n)$ is said to have a Multivariate Normal (MVN) distribution if every linear combination of the $X_j$ has a Normal distribution. That is, we require:
$$t_1X_1+ \cdots +t_kX_k$$
to have a Normal distribution for any constants $t_1, \cdots, t_k$. An important special case is $k=2$, this distribution is called the **Bivariate Normal (BVN)**.
]

- If $(X_1,\cdots,X_n)$ is MVN, then the marginal distribution of $X_1$ is Normal, since we can take $t_1$ to be 1 and all other 0.
- It is possible to have Normally distributed r.v.s $X_1,\cdots,X_n$, such that $(X_1,\cdots,X_n)$ is not Multivariate Normal.

---

**Example.** Let $X \sim N(0,1)$ and $S$ be $1$ with probability $1/2$ and $-1$ with probability $1/2$. Then it can be prove that $Y=SX$ is a standard normal. However, $(X,Y)$ is not bivariate normal because $P(X + Y = 0) = P(S = -1) = 1/2$ which implies that $X+Y$ cannot be Normal (or  have a continuous distribution). Thus, $X+Y$ is a linear combination of $X$ and $Y$ that is NOT Normally distributed, and (X,Y) is not Bivariate Normal.

---

### Bivariate Normal

.definition-box[Two r.v.s $X$ and $Y$ are said to be bivariate normal, or joint normal, if $aX + bY$ has a normal distribution for $a,b \in \mathbb{R}.$
]

.theorem-box[If $(X_1,X_2,X_3)$ is Multivariate Normal, then so is the subvector $(X_1,X_2)$.
]

To specify a bivariate normal distribution we need:

* $E(X), E(Y)$
* $Var(X), Var(Y)$
* $Corr(X,Y)$

---

### Bivariate Normal

$X$ and $Y$ are said to have a bivariate normal distribution with parameters $\mu_X$, $\sigma^2_X$, $\mu_Y$, $\sigma^2_Y$, and $\rho$, if their joint PDF is given by:
$$f_{XY}(x,y)=\frac{1}{2 \pi \sigma^2_X \sigma^2_Y \sqrt{1-\rho^2}} exp \{−\frac{1}{2(1−\rho^2)}\left(C \right) \}$$

where:

$$C = \frac{(x-\mu_X)^2}{\sigma_X^2}+ \frac{(y-\mu_Y)^2}{\sigma_Y^2}−2\rho \frac{(x-\mu_X)(y-\mu_Y)}{\sigma_Y \sigma_X}$$

---

For example, in this case, $E(X)=0$, $E(Y)=0$, $Var(X)=1$, $Var(Y)=1$, and $\rho$ is the correlation coefficient.

$$f_{XY}(x,y)=\frac{1}{2 \pi \sqrt{1-\rho^2}} exp \{−\frac{1}{2(1−\rho^2)}[x^2−2\rho xy+y^2]\}$$

.theorem-box[If X and Y are bivariate normal and uncorrelated, then they are independent.
]
 
[How does it look?](https://www.geogebra.org/m/pO4JcWPz)


---

**4.** Let $X$ and $Y$ be two independent $N(0,1)$ random variables and $Z=1+X+XY2$, $W=1+X$. Find $Cov(Z,W)$.

**5** Let $X$ and $Y$ be two jointly continuous random variables with joint PDF:
$f_{XY}(x,y)= 2$  for $y+x \leq 1, x>0,y>0$ and $0$ otherwise. Find $Cov(X,Y)$ and $\rho(X,Y)$.

**6.** Let $X$ and $Y$ be jointly (bivariate) normal, with $Var(X)=Var(Y)$. Show that the two random variables $X+Y$ and $X−Y$ are independent.

**7.** Let $X$ and $Y$ be jointly normal random variables with parameters $\mu_X=0$, $\sigma^2_X=1$, $\mu_Y=−1$, $\sigma^2_Y=4$, and $\rho=−12$.
Find $P(X+Y>0)$.
Find the constant a if we know $aX+Y$ and $X+2Y$ are independent.
Find $P(X+Y>0|2X−Y=0)$.